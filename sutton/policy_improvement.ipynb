{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article is going to discuss policy improvement building on top of previous discussion around solving the problem of gridworld.\n",
    "\n",
    "If you remember, you can solve for the value function given the \"random walk\" policy by solving the linear system below or by using policy iteration to approximate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V^{\\pi}(s) = \\sum_{a} \\pi(s,a) \\sum_{s^\\prime} P_{s{s^\\prime}}^{a} \n",
    "   (R_{s{s^\\prime}}^{a} + \\gamma V^{\\pi}(s^\\prime)) $$\n",
    "   \n",
    "$$ V^{\\pi}(s) = \\sum_{a \\in (E,S,W,N)} \\frac{1}{4} \\sum_{s^\\prime}(R_{s{s^\\prime}}^{a} + \\gamma V^{\\pi}(s^\\prime)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 4V_{00}=\\gamma V_{10}+0+\\gamma V_{01}+0+\\gamma V_{00}+-1+\\gamma V_{00}+-1 $$\n",
    "$$ 4V_{01}=\\gamma V_{11}+0+\\gamma V_{02}+0+\\gamma V_{01}+-1+\\gamma V_{00}+0 $$\n",
    "$$ 4V_{02}=\\gamma V_{12}+0+\\gamma V_{03}+0+\\gamma V_{02}+-1+\\gamma V_{01}+0 $$\n",
    "$$ 4V_{03}=\\gamma V_{13}+0+\\gamma V_{04}+0+\\gamma V_{03}+-1+\\gamma V_{02}+0 $$\n",
    "$$ 4V_{04}=\\gamma V_{14}+0+\\gamma V_{04}+-1+\\gamma V_{04}+-1+\\gamma V_{03}+0 $$\n",
    "$$ 4V_{10}=\\gamma V_{20}+0+\\gamma V_{11}+0+\\gamma V_{00}+0+\\gamma V_{10}+-1 $$\n",
    "$$ 4V_{11}=\\gamma V_{21}+0+\\gamma V_{12}+0+\\gamma V_{01}+0+\\gamma V_{10}+0 $$\n",
    "$$ 4V_{12}=\\gamma V_{22}+0+\\gamma V_{13}+0+\\gamma V_{02}+0+\\gamma V_{11}+0 $$\n",
    "$$ 4V_{13}=\\gamma V_{23}+0+\\gamma V_{14}+0+\\gamma V_{03}+0+\\gamma V_{12}+0 $$\n",
    "$$ 4V_{14}=\\gamma V_{24}+0+\\gamma V_{14}+-1+\\gamma V_{04}+0+\\gamma V_{13}+0 $$\n",
    "$$ 4V_{20}=\\gamma V_{30}+0+\\gamma V_{21}+0+\\gamma V_{10}+0+\\gamma V_{20}+-1 $$\n",
    "$$ 4V_{21}=\\gamma V_{31}+0+\\gamma V_{22}+0+\\gamma V_{11}+0+\\gamma V_{20}+0 $$\n",
    "$$ 4V_{22}=\\gamma V_{32}+0+\\gamma V_{23}+0+\\gamma V_{12}+0+\\gamma V_{21}+0 $$\n",
    "$$ 4V_{23}=\\gamma V_{33}+0+\\gamma V_{24}+0+\\gamma V_{13}+0+\\gamma V_{22}+0 $$\n",
    "$$ 4V_{24}=\\gamma V_{34}+0+\\gamma V_{24}+-1+\\gamma V_{14}+0+\\gamma V_{23}+0 $$\n",
    "$$ 4V_{30}=\\gamma V_{40}+0+\\gamma V_{31}+0+\\gamma V_{20}+0+\\gamma V_{30}+-1 $$\n",
    "$$ 4V_{31}=\\gamma V_{41}+0+\\gamma V_{32}+0+\\gamma V_{21}+0+\\gamma V_{30}+0 $$\n",
    "$$ 4V_{32}=\\gamma V_{42}+0+\\gamma V_{33}+0+\\gamma V_{22}+0+\\gamma V_{31}+0 $$\n",
    "$$ 4V_{33}=\\gamma V_{43}+0+\\gamma V_{34}+0+\\gamma V_{23}+0+\\gamma V_{32}+0 $$\n",
    "$$ 4V_{34}=\\gamma V_{44}+0+\\gamma V_{34}+-1+\\gamma V_{24}+0+\\gamma V_{33}+0 $$\n",
    "$$ 4V_{40}=\\gamma V_{40}+-1+\\gamma V_{41}+0+\\gamma V_{30}+0+\\gamma V_{40}+-1 $$\n",
    "$$ 4V_{41}=\\gamma V_{41}+-1+\\gamma V_{42}+0+\\gamma V_{31}+0+\\gamma V_{40}+0 $$\n",
    "$$ 4V_{42}=\\gamma V_{42}+-1+\\gamma V_{43}+0+\\gamma V_{32}+0+\\gamma V_{41}+0 $$\n",
    "$$ 4V_{43}=\\gamma V_{43}+-1+\\gamma V_{44}+0+\\gamma V_{33}+0+\\gamma V_{42}+0 $$\n",
    "$$ 4V_{44}=\\gamma V_{44}+-1+\\gamma V_{44}+-1+\\gamma V_{34}+0+\\gamma V_{43}+0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is mentioned in the book that one way to find a better policy at a given state is by selection a different action and thereafter following the existing policy.\n",
    "\n",
    "$$ Q^{\\pi}(s, a) = \\sum_{s'} P_{s{s^\\prime}}^{a}(R_{s{s^\\prime}}^{a} + \\gamma V^{\\pi}(s^\\prime)) $$\n",
    "   \n",
    "If the environment dynamics is fully understood and deterministic, by taken given action, the next state is known for certainty, this can be even further simplified to be. \n",
    "\n",
    "$$ Q^{\\pi}(s, a) = R_{s{s^\\prime}}^{a} + \\gamma V^{\\pi}(s^\\prime) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this could be very intersting, if you pay attention to the definition of $V^\\pi(s)$ and $Q^\\pi(s, a)$, we know:\n",
    "\n",
    "$$ V^{\\pi}(s) = \\sum_{a} \\pi(s,a) Q^\\pi(s, a)  $$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a random cell and validate if the above equation hold true. Here let's look into the cell of (0, 2) which is the cell in the middle on the most left column. \n",
    "\n",
    "It has the following possible actions which leads to its corresponding next state:\n",
    "\n",
    "- S = [0,2] ---> a = E, r = 0  ---> S' = [1,2]\n",
    "- S = [0,2] ---> a = S, r = 0  ---> S' = [0,3]\n",
    "- S = [0,2] ---> a = W, r = -1  ---> S' = [0,2]   (hit the call, bounce back)\n",
    "- S = [0,2] ---> a = N, r = 0  ---> S' = [0,1]\n",
    "\n",
    "We know that $Q^{\\pi}(s, a) = R_{s{s^\\prime}}^{a} + \\gamma V^{\\pi}(s^\\prime)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the value function for the given policy of random walk \n",
    "\n",
    "||c0|c1|c2|c3|c4|\n",
    "|-|-|-|-|-|-|\n",
    "|r0|3.282|8.762|4.097|4.403|0.998|\n",
    "|r1|1.483|2.922|2.058|1.558|0.258|\n",
    "|r2|0.015|0.682|0.569|0.204|-0.556|\n",
    "|r3|-1.002|-0.475|-0.414|-0.663|-1.266|\n",
    "|r4|-1.882|-1.375|-1.27|-1.473|-2.029|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the Q functions for all the possible actions for state (0,2), we have \n",
    "\n",
    "- $Q^{\\pi}(s=[0,2], a=E) = R_{s=[0,2]{s^\\prime=[1,2]}}^{a=E} + \\gamma V^{\\pi}(s^\\prime=[1,2]) = 0 + 0.9 * V^{\\pi}(s^\\prime=[1,2]) = 0 + 0.9 * 0.682 = 0.614 $\n",
    "- $Q^{\\pi}(s=[0,2], a=S) = R_{s=[0,2]{s^\\prime=[0,3]}}^{a=S} + \\gamma V^{\\pi}(s^\\prime=[0,3]) = 0 + 0.9 * V^{\\pi}(s^\\prime=[0,3]) = 0 + 0.9 * (-1.002) = -0.912 $\n",
    "- $Q^{\\pi}(s=[0,2], a=W) = R_{s=[0,2]{s^\\prime=[0,2]}}^{a=W} + \\gamma V^{\\pi}(s^\\prime=[0,2]) = -1 + 0.9 * V^{\\pi}(s^\\prime=[0,2]) = -1 + 0.9 * 0.015 = -0.987$\n",
    "- $Q^{\\pi}(s=[0,2], a=N) = R_{s=[0,2]{s^\\prime=[0,1]}}^{a=N} + \\gamma V^{\\pi}(s^\\prime=[0,1]) = 0 + 0.9 * V^{\\pi}(s^\\prime=[0,1]) = 0 + 0.9 * 1.483 = 1.335$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the random policy, we know we have a equal probability of taking either action. so we have \n",
    "\n",
    "$$ V^{\\pi}(s=[0,2]) = \\frac{Q^{\\pi}(s=[0,2], a=E) + Q^{\\pi}(s=[0,2], a=S) + Q^{\\pi}(s=[0,2], a=W) + Q^{\\pi}(s=[0,2], a=N)}{4} = 0.01505 \\approx 0.015 $$\n",
    "\n",
    "Now looking at the Q value of all the possible actions, we can tell that heading North has the highest Q value of 1.335. So what does this mean? \n",
    "\n",
    "So first, let's keep in mind that the State value function is based on the assumption of a given policy. And in this case, all the value functions above were calculated under the assumption of \"random walk\", if we change our policy now to be \"whenever you are on cell (0,2), instead of randomly walk, head North because it will get a higher state function\". Actually, this is now a different policy overall. \n",
    "\n",
    "Let's first start by looking at two scenarios:\n",
    "\n",
    "### Scenario 1\n",
    "Between two policies, the only difference is that \"when you **FIRST** land on S=(0,2), go north but for the future landing on (0,2), still random walk for the rest of the game\".\n",
    "\n",
    "One has to be aware of what determines the value function, it is the policy and the dynamics of the environment.   \n",
    "\n",
    "\n",
    "### Scenario 2\n",
    "Between two policies, the only difference is that \"**EVERY TIME** you land on S=(0,2), go north\". The change in the policy will change the linear equation that we created in the first notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.808  9.111  4.326  4.553  1.106]\n",
      " [ 2.419  3.458  2.347  1.723  0.372]\n",
      " [ 2.177  1.493  0.924  0.385 -0.437]\n",
      " [-0.057  0.074 -0.118 -0.497 -1.151]\n",
      " [-1.336 -0.987 -1.028 -1.323 -1.921]]\n"
     ]
    }
   ],
   "source": [
    "size = 5\n",
    "coef = np.zeros(shape=(size*size,size*size))\n",
    "res = [0 for i in range(size*size)]\n",
    "gamma = 0.9\n",
    "\n",
    "# loop through each column\n",
    "for x in range(size):\n",
    "    \n",
    "    # loop through each row\n",
    "    for y in range(size):\n",
    "        \n",
    "        equation_num = x*size+y\n",
    "        # skip teleport points for now, deal with later \n",
    "        # the two teleport points: S=[1,0], S=[3,0], S=[0,2]\n",
    "        if equation_num in (5, 15, 2):\n",
    "            continue\n",
    "        \n",
    "        # top-left:0, bot-left:4, top-right:20, bot-right:24\n",
    "        cell_num = equation_num \n",
    "        coef[equation_num][cell_num] = 4  \n",
    "        \n",
    "        # E: x+1, y\n",
    "        r = 0\n",
    "        x_ = x+1\n",
    "        y_ = y \n",
    "        if x_ > size - 1:\n",
    "            x_ = x\n",
    "            r = -1 \n",
    "        coef[equation_num][x_*size+y_] -= gamma \n",
    "        res[equation_num] += r\n",
    "        \n",
    "        # S: x, y+1\n",
    "        r = 0\n",
    "        x_ = x\n",
    "        y_ = y+1 \n",
    "        if y_ > size - 1:\n",
    "            y_ = y\n",
    "            r = -1\n",
    "        coef[equation_num][x_*size+y_] -= gamma \n",
    "        res[equation_num] += r\n",
    "\n",
    "\n",
    "        # W: x-1, y\n",
    "        r = 0\n",
    "        x_ = x-1\n",
    "        y_ = y \n",
    "        if x_ < 0:\n",
    "            x_ = x\n",
    "            r = -1\n",
    "        coef[equation_num][x_*size+y_] -= gamma \n",
    "        res[equation_num] += r\n",
    "        # N: x, y-1\n",
    "        r = 0\n",
    "        x_ = x\n",
    "        y_ = y-1 \n",
    "        if y_ < 0:\n",
    "            y_ = y\n",
    "            r = -1\n",
    "        coef[equation_num][x_*size+y_] -= gamma \n",
    "        res[equation_num] += r\n",
    "        \n",
    "\n",
    "# V(s=[1,0])\n",
    "equation_num = 1 * size + 0 \n",
    "coef[equation_num][1 * size + 0] = 1\n",
    "coef[equation_num][1 * size + 4] -= gamma\n",
    "res[equation_num] = 10\n",
    "\n",
    "# V(s=[3,0])\n",
    "equation_num = 3 * size + 0 \n",
    "coef[equation_num][3 * size + 0] = 1 \n",
    "coef[equation_num][3 * size + 3] -= gamma \n",
    "res[equation_num] = 5\n",
    "\n",
    "# V(s=[0,2])\n",
    "equation_num = 0 * size + 2 \n",
    "coef[equation_num][0 * size + 2] = 1 \n",
    "coef[equation_num][0 * size + 1] -= gamma \n",
    "res[equation_num] = 0\n",
    "\n",
    "v = np.linalg.solve(coef, res)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "t = v.copy()\n",
    "t.shape=(5,5)\n",
    "print(t.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Difference between the new policy and old policy\n",
      ">>> Positive means the value function became bigger in the new policy\n",
      "[[ 0.526 -6.343 -1.92  -4.46  -2.334]\n",
      " [ 7.628  0.536 -0.565 -1.484 -1.245]\n",
      " [ 4.311  1.665  0.355 -0.322 -0.472]\n",
      " [ 5.555  2.198  0.799  0.166 -0.057]\n",
      " [ 2.988  1.747  0.833  0.322  0.108]]\n"
     ]
    }
   ],
   "source": [
    "v_random_walk = [[ 3.282,  8.762,  4.097,  4.403,  0.998],\n",
    " [ 1.483,  2.922,  2.058,  1.558,  0.258],\n",
    " [ 0.015,  0.682,  0.569,  0.204, -0.556],\n",
    " [-1.002, -0.475, -0.414, -0.663, -1.266],\n",
    " [-1.882, -1.375, -1.27,  -1.473, -2.029]]\n",
    "\n",
    "print(\">>> Difference between the new policy and old policy\")\n",
    "print(\">>> Positive means the value function became bigger in the new policy\")\n",
    "print(t-v_random_walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we only change one step in the policy, however, it changed all the state value functions. And if you take a look, the value function got decreased for all the cells above the diagonal. And the ones on the diagonal and the below got increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.808  3.808  2.808  2.808]\n",
      "[1 1 0 0]\n",
      "[ 0.5  0.5  0.   0. ]\n",
      "[ 2.419  2.419  1.419  2.419]\n",
      "[1 1 0 1]\n",
      "[ 0.333  0.333  0.     0.333]\n",
      "[ 2.177  2.177  1.177  2.177]\n",
      "[1 1 0 1]\n",
      "[ 0.333  0.333  0.     0.333]\n",
      "[-0.057 -0.057 -1.057 -0.057]\n",
      "[1 1 0 1]\n",
      "[ 0.333  0.333  0.     0.333]\n",
      "[-1.336 -2.336 -2.336 -1.336]\n",
      "[1 0 0 1]\n",
      "[ 0.5  0.   0.   0.5]\n",
      "[ 19.111  19.111  19.111  19.111]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 3.458  3.458  3.458  3.458]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 1.493  1.493  1.493  1.493]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 0.074 -0.926  0.074  0.074]\n",
      "[1 0 1 1]\n",
      "[ 0.333  0.     0.333  0.333]\n",
      "[-0.987 -0.987 -0.987 -0.987]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 4.326  4.326  4.326  3.326]\n",
      "[1 1 1 0]\n",
      "[ 0.333  0.333  0.333  0.   ]\n",
      "[ 2.347  2.347  2.347  2.347]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 0.924 -0.076  0.924  0.924]\n",
      "[1 0 1 1]\n",
      "[ 0.333  0.     0.333  0.333]\n",
      "[-0.118 -0.118 -0.118 -0.118]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[-1.028 -1.028 -1.028 -1.028]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 9.553  9.553  9.553  9.553]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 1.723  0.723  1.723  1.723]\n",
      "[1 0 1 1]\n",
      "[ 0.333  0.     0.333  0.333]\n",
      "[ 0.385  0.385  0.385  0.385]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[-0.497 -0.497 -0.497 -0.497]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[-1.323 -1.323 -1.323 -1.323]\n",
      "[1 1 1 1]\n",
      "[ 0.25  0.25  0.25  0.25]\n",
      "[ 0.106  0.106  1.106  0.106]\n",
      "[0 0 1 0]\n",
      "[ 0.  0.  1.  0.]\n",
      "[-0.628  0.372  0.372  0.372]\n",
      "[0 1 1 1]\n",
      "[ 0.     0.333  0.333  0.333]\n",
      "[-1.437 -0.437 -0.437 -0.437]\n",
      "[0 1 1 1]\n",
      "[ 0.     0.333  0.333  0.333]\n",
      "[-2.151 -1.151 -1.151 -1.151]\n",
      "[0 1 1 1]\n",
      "[ 0.     0.333  0.333  0.333]\n",
      "[-2.921 -2.921 -1.921 -1.921]\n",
      "[0 0 1 1]\n",
      "[ 0.   0.   0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "actions = ['E', 'S', 'W', 'N']\n",
    "# a table where each row is a unique state, and each column represent an action\n",
    "policy_probability_table = np.zeros(shape=(size*size, len(actions))) + 1/len(actions)\n",
    "qtable = np.zeros(shape=(size*size, len(actions)))\n",
    "v_copy = v.copy()\n",
    "\n",
    "# the value function will change after you change your policy \n",
    "# however, the reward will be solely dependent on the state and action\n",
    "# and we can precalculate the reward as a function of state, action \n",
    "\n",
    "def calc_reward(state, action):\n",
    "    # if A = [1,0] teleport\n",
    "    if state == 5:\n",
    "        return 10\n",
    "    # if B = [3,0] teleport\n",
    "    elif state == 15:\n",
    "        return 5\n",
    "    # if edge\n",
    "    elif (\n",
    "        # east [4,0] 20, [4,1] 21, ..., [4,4] 24\n",
    "        (int(state/5) == 4 and action == 'E') or\n",
    "        # west [0,0] 0, [0,1] 1, ..., [0,4] 4\n",
    "        (int(state/5) == 0 and action == 'W') or\n",
    "        # south [0,4] 4, [1,4] 8, ..., [4,4] 24\n",
    "        (state%4 == 0 and state != 0 and action == 'S') or\n",
    "        # north [0,0] 0, [1,0] 5, ..., [4,0] 20\n",
    "        (state%5 == 0 and action == 'N')\n",
    "    ):\n",
    "        return -1 \n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "    \n",
    "numb_iterations = 1\n",
    "for _ in range(num_iterations):\n",
    "    for state in range(size*size):\n",
    "        \n",
    "        # initialize\n",
    "        for action in actions:\n",
    "            # R \n",
    "            reward = calc_reward(state, action)\n",
    "            # Q=R+V\n",
    "            q = reward + v_copy[state]\n",
    "            action_idx = actions.index(action)\n",
    "            qtable[state][action_idx] = q\n",
    "        \n",
    "        max_q = qtable[state].max()\n",
    "        \n",
    "        # set policy_probability_table to be the greedy\n",
    "        # the probability of the optimal actions are 1/(# of optimal actions)\n",
    "        qs = np.array([1 if e == max_q else 0 for e in qtable[state]])\n",
    "        qs = qs / qs.sum()\n",
    "        # choose greedy action, update policy for all states \n",
    "        policy_probability_table[state] = qs\n",
    "    \n",
    "    # new V\n",
    "    # generate the linear system, solve it in order to get V\n",
    "    \n",
    "    break\n",
    "    #repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-3917489b6177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
