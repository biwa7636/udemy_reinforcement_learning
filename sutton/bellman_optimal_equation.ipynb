{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another notebook that on top of the previous one Bellman Equation State Value Function, meant to solve the gridworld problem and calculating all the optimal value functions along with all the optimal action state value functions in order to identify \"what is the right action to take\". \n",
    "\n",
    "Credit: Reinforcement Learning - Sutton & Barto \n",
    "Page 75 Optimal Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimal State-Value Function: $ V^*(s) = \\underset{\\pi}{max} V^\\pi(s)$\n",
    "- Optimal Action-Value Function: $ Q^*(s, a) = \\underset{\\pi}{max} Q^\\pi(s, a)$\n",
    "- Q in terms of V: $ Q^*(s, a)= E \\left \\{ r_{t+1} + \\gamma V^*(s_{t+1})\\, \\middle| \\, s_t = s, a_t = a \\right \\}$\n",
    "- Bellman Optimality Equation\n",
    "$$ V^*(s) = \\underset{\\pi}{max} E \\left \\{r_{t+1} + \\gamma V^*(s_{t+1})\\, \\middle| \\, s_t=s, a_t = a\\right \\}$$\n",
    "$$ V^*(s) = \\underset{\\pi}{max} \\underset{s'}{\\sum} P^a_{ss'} [R^a_{ss'} + \\gamma V^*(s')] $$\n",
    "\n",
    "$$ Q^*(s, a) = E \\left \\{r_{t+1} + \\gamma \\underset{a'}{max} Q^*(s_{t+1},a')\\, \\middle| \\, s_t=s, a_t = a\\right \\}$$\n",
    "$$ Q^*(s, a) =  \\underset{s'}{\\sum} P^a_{ss'} [R^a_{ss'} + \\gamma \\underset{a'}{max} Q^*(s', a')] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to solve for $V^*(s)$ for all states, and then one can solve a related set of equations for $Q^*$. However, before we proceed, I want to highlight what is the difference between this notebook and the previous. \n",
    "\n",
    "The previous one was to calculate the value function **given** a policy. To refresh the memory, the given policy was a randomly walk where the agent has equal probability of going to each direction. That was one policy and that was the only policy. For a different policy, say the agent has higher change of going to one direction versus the others based on whatever condition, let's will surely lead to a different value function for each state. In this example, theoretically, we are going to find the one out of many, hence, the best one out of potentially infinite amount of policies that could potentially exist.  \n",
    "\n",
    "To make the discussion make more sense, let's first try to find the optimal state value function for the state where the agent is at the top left corner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ V^*(s) = \\underset{\\pi}{max} E \\left \\{r_{t+1} + \\gamma V^*(s_{t+1})\\, \\middle| \\, s_t=s, a_t = a\\right \\}$$\n",
    "\n",
    "$$ V^*(s=[0,0]) = \\underset{\\pi}{max} E \\left \\{r_{t+1} + \\gamma V^*(s_{t+1})\\, \\middle| \\, s_t=s, a_t = a\\right \\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the actions that we could possibly take? only four (E, S, W, N). However, how many policies are there for $\\pi(s=[0,0], a)$? infinite. The only thing that is required for a qualified policy for this particular state is that the sum of all the probabilities of choosing each action add up to 1 and that's all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\pi(s=[0,0],a=E) + \\pi(s=[0,0],a=S) + \\pi(s=[0,0],a=W) + \\pi(s=[0,0],a=N) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, can we have one or more than one combination of the probability of going to each direction so that we know "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
